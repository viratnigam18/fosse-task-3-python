# Research Plan — Evaluating Open Source Models for Student Competence Analysis

## Paragraph 1 — Approach
I will identify a small set of freely available code-capable LLMs (such as Code Llama, StarCoder2, and Falcon variants) and shortlist two for practical evaluation based on licensing, model size (inference cost), and documented code abilities. For each chosen model I will run a battery of targeted prompts designed to (a) summarize student code, (b) ask conceptual questions that probe for understanding, and (c) generate scaffolded hints for debugging without revealing full fixes. The dataset for tests will include short student-submitted Python snippets (typical undergraduate assignments, code with common bugs like off-by-one, wrong indent, misuse of lists vs. dicts) and a few canonical tasks from public code-eval sets to calibrate behavior.

## Paragraph 2 — Validation and Metrics
I will validate models by measuring: (1) **helpfulness** — how often the model’s hints lead a human grader to identify the bug (measured via binary success on a small human trial), (2) **non-revelation** — whether the model refuses to produce the full solution when prompted, (3) **explainability** — clarity and correctness of conceptual explanations (human-rated), and (4) **efficiency** — latency and token cost. I will run each model through the same prompt template, log responses, and perform a small human evaluation (3–5 graders) to score responses on a 1–5 rubric for helpfulness and pedagogy. Based on these results I will recommend whether the model is suitable as an automated aid or needs additional fine-tuning/guardrails.
